// Essential Configuration ------------------------------------------------------------
const NEWRELIC_DC = "US";        // datacenter for account - US or EU
const SRC_ACCOUNT_ID = "1234567"    //Account ID of account tosource data from
const SRC_QUERY_KEY = "NRAK-..."    // User API Key for reading data (Please provide with secure credential if possible.)
const DEST_INSERT_KEY="...FNRAL"    // Ingest API Key for sending data (Please provide with secure credential if possible.) //e.g. $secure.YOU_CREDENTIAL_NAME

// Configure these values only if your destination account is different to that in which data is sourced.
const DEST_ACCOUNT_ID = SRC_ACCOUNT_ID;  // Account ID to record data back to
const DEST_QUERY_KEY = SRC_QUERY_KEY;    // User API Key of the destination account.



// Optional Configuration ------------------------------------------------------------
const MONITOR_NAME="EM2M " ;    // the monitor name, only really relevant if deploying more than monitor
const DEFAULT_TIMEOUT = 15000   // default timeout for queries, in ms
const NAMESPACE ="em2m"         // metric names are prefixed with this
const DEFAULT_REHYDRATE_LOOKBACK= 125;      // minutes to rehydrate from if no pre-existing data
const DEFAULT_TIME_UNTIl_NOW_BUFFER= 10;    // minutes from now to cease lookup (as data may not have arrived yet to be queried)
const MAX_BUCKETS_PER_QUERY=200;            // max number of data points to request at once (366 is max supported by timeseries)
const MAX_METRICS_PER_SEND_BATCH=2000       // how many metrics to send to new relic in a single payload (batching occurs)

const INGEST_METRIC_ENDPOINT = NEWRELIC_DC === "EU" ? "metric-api.eu.newrelic.com" : "metric-api.newrelic.com" 
const GRAPHQL_ENDPOINT = NEWRELIC_DC === "EU" ? "api.eu.newrelic.com" : "api.newrelic.com" 


// Task configuration -------------------------------------------------------------
const TASKS = [
{
    "id":"example1",            // a unique ID for this task
    "metricName":"example1cpu", // Name of the metric, this will be prefixed with the namespace "em2m."
    "offsetFromNowBuffer": 5,   // minutes - data fresher than this will be ignored (and picked up in next run)
    "rehydrateLookback": 60,    // minutes - If never run or last run not found, how far to go back an retrieve data (max 48hrs)
    "accountId":"1234567",      // Account ID to gather data from
    "bucketSize": 60,           // The size of bucket in seconds. e.g. 60 = 1 minute, data will be aggregated in 1 minute blocks.
    "selector":["max","min","records","percentile.95","average","processorCount"],  // The fields from the query (below) to record as metrics. 
    "query":`FROM SystemSample select max(cpuPercent) as max, min(cpuPercent) as min, count(*) as records, percentile(cpuPercent,95) as percentile, average(cpuPercent) as average, average(numeric(processorCount)) as processorCount  where cpuPercent is not null facet hostname, entityGuid`,
}]


// End Configuration ---------------------------------------------------------------

let assert=require("assert"),_=require("lodash"),RUNNING_LOCALLY=!1;const IS_LOCAL_ENV=void 0===$http;var $http;function getRoundedDateUnixTime(e,t=0,a=new Date){e*=6e4,t=1e3*t*60;return new Date(Math.floor((a.getTime()-t)/e)*e).getTime()}function formatDateFromUnix(e){e=new Date(e);return e.getFullYear()+"-"+(""+(e.getMonth()+1)).padStart(2,"0")+"-"+(""+e.getDate()).padStart(2,"0")+" "+(""+e.getHours()).padStart(2,"0")+":"+(""+e.getMinutes()).padStart(2,"0")}async function asyncForEach(t,a){for(let e=0;e<t.length;e++)await a(t[e],e,t)}function isObject(e){return null!==e&&("function"==typeof e||"object"==typeof e)}IS_LOCAL_ENV&&(RUNNING_LOCALLY=!0,$http=require("request"),console.log("Running in local mode",!0));const genericServiceCall=function(e,s,n){"timeout"in s||(s.timeout=DEFAULT_TIMEOUT);let i="number"==typeof e?[e]:e;return new Promise((r,o)=>{$http(s,function(e,t,a){e?(console.log(`Error: Connection error on url '${s.url}'`),o(`Connection error on url '${s.url}'`)):i.includes(t.statusCode)?r(n(a,t,e)):(a=`Expected [${i}] response code but got '${t.statusCode}' from url '${s.url}'`,o(a))})})},setAttribute=function(e,t){RUNNING_LOCALLY||$util.insights.set(e,t)},sendDataToNewRelic=async e=>{e={url:`https://${INGEST_METRIC_ENDPOINT}/metric/v1`,method:"POST",headers:{"Api-Key":DEST_INSERT_KEY},body:JSON.stringify(e)};return genericServiceCall([200,202],e,(e,t,a)=>!a||(console.log(`NR Post failed : ${a} `,!0),!1))},processTimeseriesData=(e,t,o)=>{const s=e?.nrql?.results,n=e?.nrql?.metadata?.facets;let i=[],c=[];if(c=(c=Array.isArray(t)?t:[t]).filter(e=>{return void 0!==_.get(s[0],e)||(console.log(`Warning: the selector '${e}' does not appear to exist as a column in the result set. Check your task configuration. Skipping.`),!1)}),s)return s.forEach(a=>{let r={};n&&n.forEach((e,t)=>{Array.isArray(a.facet)?r[e]=a.facet[t]:r[e]=a.facet}),c.forEach(e=>{var t=_.get(s[0],e);void 0===t?console.log(`Warning: value for field '${e}' undefined, skiping`):i.push({name:NAMESPACE+`.${o}.`+e,type:"gauge",value:t,timestamp:a.beginTimeSeconds,attributes:r})})}),i;throw console.log("Error: No results to process"),"No results to process"},prepareMetricPayload=(e,t)=>{var a={attributes:{}},e=(a.attributes[NAMESPACE+".monitorName"]=MONITOR_NAME,a.attributes[NAMESPACE+".task.Id"]=e,a.attributes.source=NAMESPACE,[{common:a,metrics:t}]);return e},JSONParseGraphQLResponse=e=>{try{return isObject(e)?e:JSON.parse(e)}catch(e){throw console.log("JSON parse failed"),e}},queryNRQL=async(e,t,a)=>{t={url:`https://${GRAPHQL_ENDPOINT}/graphql`,method:"POST",headers:{"Content-Type":"application/json","API-Key":t},body:JSON.stringify({query:`{ actor { account(id: ${e}) { nrql(query: "${a}") { results metadata { facets } } } } }  `})};try{var r=await genericServiceCall([200],t,e=>e);return JSONParseGraphQLResponse(r)}catch(e){throw e}},getLastRunTimestamps=async e=>{console.log("Determing hydration start times...");var t=e.map(e=>`'${e.id}'`),t=`select latest(timestamp) as latestTimestamp from Metric RAW since 48 hours ago facet ${NAMESPACE}.task.Id as 'taskId' where ${NAMESPACE}.task.Id in (${t.toString()}) limit max`;const r=(await queryNRQL(DEST_ACCOUNT_ID,DEST_QUERY_KEY,t,"Gather last run by task"))?.data?.actor?.account?.nrql?.results;e.forEach(t=>{let e;var a;(e=r?r.find(e=>""+t.id===e.taskId):e)?(t.lastRunTimestamp=e.latestTimestamp+1e3*t.bucketSize,console.log(t.id+": Last run discovered, rehydrating from "+formatDateFromUnix(t.lastRunTimestamp))):(a=t.rehydrateLookback||DEFAULT_REHYDRATE_LOOKBACK,t.lastRunTimestamp=getRoundedDateUnixTime(t.bucketSize/60,a),console.log(t.id+": No previous data found, rehydrating from "+formatDateFromUnix(t.lastRunTimestamp)))})},deriveLookupQueries=t=>{var a=getRoundedDateUnixTime(t.bucketSize/60,t.offsetFromNowBuffer||DEFAULT_TIME_UNTIl_NOW_BUFFER),e=(a-t.lastRunTimestamp)/(1e3*t.bucketSize),r=Math.ceil(e/MAX_BUCKETS_PER_QUERY),o=(console.log(`Query time window: ${formatDateFromUnix(t.lastRunTimestamp)} until `+formatDateFromUnix(a)),console.log("Buckets in window:",Math.ceil(e)),console.log("Query batches:",r),[]);let s=t.lastRunTimestamp;for(let e=0;e<r;e++){var n=a<(n=s+MAX_BUCKETS_PER_QUERY*(1e3*t.bucketSize))?a:n;o.push({query:t.query+` SINCE ${s} UNTIL ${n} TIMESERIES ${t.bucketSize} seconds`,sinceTime:s,untilTime:n}),s=n}t.queries=o},processTaskQueries=async o=>{console.log("");let s=[];return await asyncForEach(o.queries,async(e,t)=>{try{console.log(`Querying batch ${t+1}/${o.queries.length} ...  ${formatDateFromUnix(e.sinceTime)} until `+formatDateFromUnix(e.untilTime));var a=await queryNRQL(o.accountId,SRC_QUERY_KEY,e.query),r=await processTimeseriesData(a?.data?.actor?.account,o.selector,o.metricName);s=[...s,...r]}catch(e){throw console.log("Error fetching data for this batch"),e}}),console.log("Metrics retrieved:",s.length),s},sendDataToNewRelicInBatches=async(t,a)=>{var r=Math.ceil(a.length/MAX_METRICS_PER_SEND_BATCH);console.log("Data send batches:",r);for(let e=0;e<r;e++){var o=a.slice(e*MAX_METRICS_PER_SEND_BATCH,e*MAX_METRICS_PER_SEND_BATCH+MAX_METRICS_PER_SEND_BATCH);console.log(`Sending batch ${e+1}/${r}...`);try{await sendDataToNewRelic(prepareMetricPayload(t.id,o))}catch(e){throw"Data send to NR failed"}}};async function runtasks(e){let a={TOTAL_TASKS:e.length,ATTEMPTED_TASKS:0,SUCCESSFUL_TASKS:0,FAILED_TASKS:0};await getLastRunTimestamps(e);try{await asyncForEach(e,async e=>{a.ATTEMPTED_TASKS++,console.log(`
[Task ${e.id}]---------------`),deriveLookupQueries(e);try{var t=await processTaskQueries(e);await sendDataToNewRelicInBatches(e,t),a.SUCCESSFUL_TASKS++}catch(e){console.log("Error: Something went wrong with this task marking as failed."),a.FAILED_TASKS++}})}catch(e){console.log("An error occured:",e)}return a}try{setAttribute("totalTasksConfigured",TASKS.length),runtasks(TASKS).then(e=>{console.log("\n\n---------------------"),console.log("Task completion summary:",e),setAttribute("taskRunComplete","YES"),setAttribute("taskTotal",e.TOTAL_TASKS),setAttribute("taskFailed",e.FAILED_TASKS),setAttribute("taskSuccess",e.SUCCESSFUL_TASKS),setAttribute("taskAttempted",e.ATTEMPTED_TASKS),0<e.FAILED_TASKS?(setAttribute("taskResult","FAILED"),assert.fail("Not all tasks ran successfully")):(setAttribute("taskResult","SUCCESS"),console.log("Script complete."),assert.ok("All tasks passed"))})}catch(e){console.log("Unexpected errors: ",e)}
